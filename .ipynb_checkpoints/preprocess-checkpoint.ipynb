{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(filename):\n",
    "    fp = open(filename)\n",
    "    # do stuff here\n",
    "    line = fp.readline()\n",
    "    cnt = 1\n",
    "   \n",
    "    x=[]\n",
    "    y=[]\n",
    "    max_length = 0\n",
    "    while line:\n",
    "       lst = line.strip().split()\n",
    "       # first two items in the line are indices\n",
    "       temp_sentence = lst[2:-1]\n",
    "       max_length = max(max_length,len(temp_sentence))\n",
    "\n",
    "       if len(temp_sentence) != 0:\n",
    "            sentence = ' '.join(temp_sentence[:-1])\n",
    "            # # stop part of the last string\n",
    "            # sentence += temp_sentence[-1]\n",
    "            category = lst[-1]\n",
    "            x.append(sentence)\n",
    "            y.append(category)\n",
    "       line = fp.readline()\n",
    "       cnt += 1\n",
    "    fp.close()\n",
    "    assert(len(x) == len(y))\n",
    "    \n",
    "    # the col name is in the first place for y, but what about x ?\n",
    "    x = x[1:]\n",
    "    y = y[1:]\n",
    "\n",
    "    # modify Y to be 0 or 1\n",
    "    # 1 - positive, 0 - negative\n",
    "#     y = [1 if int(i) >= 3 else 0 for i in y]\n",
    "    y = [int(i) for i in y]\n",
    "    return (x, y, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, max_length):\n",
    "\n",
    "    # prepare tokenizer\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(x)\n",
    "    vocab_size = len(t.word_index) + 1\n",
    "\n",
    "    # integer (unique) encode the documents\n",
    "    # pad documents to a max_length words\n",
    "    encode_x = t.texts_to_sequences(x)\n",
    "    padded_x = pad_sequences(encode_x, maxlen=max_length, padding='post')\n",
    "\n",
    "    return padded_x, vocab_size, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train: The number of lines are:  156062\n",
    "# Test: The number of lines are:  66294\n",
    "# 70/30 split of the total data\n",
    "\n",
    "def model():\n",
    "  \n",
    "    x_train, y_train, max_length_train = getData('data/train.tsv')\n",
    "\n",
    "    # use 10% of the data as test data at the end\n",
    "\n",
    "    split = floor(len(x_train)*0.9)\n",
    "\n",
    "    x_train = x_train[:split]\n",
    "    y_train = y_train[:split]\n",
    "    padded_x_train, vocab_size_train, train_tokenizer = preprocess(x_train, max_length_train)\n",
    "\n",
    "    x_test = x_train[split:]\n",
    "    y_test = y_train[split:]\n",
    "\n",
    "    max_length_test = 0\n",
    "\n",
    "    for phrase in x_test:\n",
    "        max_length_test = max(max_length_test, len(phrase.split()))\n",
    "    \n",
    "    padded_x_test, _ ,_ = preprocess(x_test, max_length_test)\n",
    "\n",
    "    # load the whole embedding into memory\n",
    "    print(\"Loading glove data ...\")\n",
    "    \n",
    "    # 400000 word vectors\n",
    "    embeddings_index = dict()\n",
    "    fp = open('data/glove.6B.100d.txt')\n",
    "    for line in fp:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    fp.close()\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    # create a weight matrix for words in training docs\n",
    "    embedding_matrix = zeros((vocab_size_train, 100))\n",
    "    \n",
    "    total_words = 0    \n",
    "    found_words = 0\n",
    "    for word, i in train_tokenizer.word_index.items():\n",
    "        total_words += 1\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            found_words += 1\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    print(\"The number of total words for training data: \", total_words)\n",
    "    print(\"The number of found words from matrix for training data: \", found_words)\n",
    "    print(\"The fraction found: \", str((found_words/total_words)))\n",
    "\n",
    "\n",
    "    # create RNN model\n",
    "\n",
    "    # define model\n",
    "    use_dropout = False\n",
    "    model = Sequential()\n",
    "    # CORRESPONDS TO THE DIMENSION OF THE EMBEDDED MATRIX\n",
    "    hidden_size = 100\n",
    "    num_epochs = 3\n",
    "    \n",
    "\n",
    "    e = Embedding(vocab_size_train, hidden_size, weights=[embedding_matrix], input_length=max_length_train, trainable=False)\n",
    "    model.add(e)\n",
    "#     model.add(LSTM(hidden_size, return_sequences=True))\n",
    "#     model.add(LSTM(hidden_size, return_sequences=False))\n",
    "    model.add(Bidirectional(LSTM(hidden_size, return_sequences=False)))\n",
    "    # model.add(LSTM(hidden_size))\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "    model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "\n",
    "    # if use_dropout:\n",
    "    #     model.add(Dropout(0.5))\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # https://keras.rstudio.com/reference/fit.html\n",
    "    # batch_size = 32\n",
    "    # shuffle = TRUE\n",
    "    hist = model.fit(padded_x_train, y_train, epochs=num_epochs, validation_split = 0.2)\n",
    "\n",
    "    scores = model.evaluate(padded_x_test, y_test, verbose=1)\n",
    "    print('Test accuracy:', scores[1])\n",
    "\n",
    "    print(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading glove data ...\n",
      "Loaded 194504 word vectors.\n",
      "The number of total words for training data:  0\n",
      "The number of found words from matrix for training data:  0\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3876033efa86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-94e72024fadb>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The number of total words for training data: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The number of found words from matrix for training data: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfound_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The fraction found: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_words\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
